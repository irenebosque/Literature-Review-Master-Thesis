\section{Conclusions}


Sections \ref{section:Reinforcement-Learning}, \ref{section:Imitation-Learning} and \ref{section:Towards-off-policy-CIL} present the theoretical relevant  background for this master thesis. Specifically, section \ref{section:Reinforcement-Learning} introduces reinforcement learning and how it makes use of MDPs as a framework for sequential decision making problems. The review continues explaining what the terms on-policy and off-policy mean in RL with the help of SARSA and Q-learning algorithms. This section ends with the concept of experience replay, a technique  that is a critical part of this thesis. In section \ref{section:Imitation-Learning} we introduce the concept of imitation learning, a machine learning method that leverages human knowledge in the learning process being more efficient than pure autonomous learning approaches in complex real-world problems. We present the definitions found in the literature of the terms on-policy and off-policy in imitation learning together with our interpretation with which several IL algorithms are classified.
This literature review ends with section \ref{section:Towards-off-policy-CIL} where we explain in more detail the algorithm COACH and its deep version D-COACH. There, it is explained why D-COACH needs to be transformed into an off-policy algorithm to fully leverage experience replay. Finally, we present the proposal of how to carry on this transformation by introducing a new model in the D-COACH framework that will predict the human's feedback.



